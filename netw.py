import os
import mlflow
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from mlflow.tracking import MlflowClient
from streamlit_drawable_canvas import st_canvas
from datetime import datetime
import time
import requests
import io
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
import gc
import keras_tuner as kt

# H√†m ch·ªçn tham s·ªë t·ªëi ∆∞u d·ª±a tr√™n s·ªë m·∫´u
def get_optimal_params(num_samples):
    if num_samples <= 1000:
        return {
            "hidden_layer_sizes": (32,),
            "learning_rate": 0.001,
            "epochs": 30,
            "activation": "relu",
            "solver": "adam",
            "batch_size": 32
        }
    elif num_samples <= 10000:
        return {
            "hidden_layer_sizes": (64, 32),
            "learning_rate": 0.0005,
            "epochs": 50,
            "activation": "relu",
            "solver": "adam",
            "batch_size": 64
        }
    elif num_samples <= 50000:
        return {
            "hidden_layer_sizes": (128, 64),
            "learning_rate": 0.0003,
            "epochs": 70,
            "activation": "relu",
            "solver": "adam",
            "batch_size": 128
        }
    else:
        return {
            "hidden_layer_sizes": (128, 64, 32),
            "learning_rate": 0.0001,
            "epochs": 100,
            "activation": "relu",
            "solver": "adam",
            "batch_size": 256
        }

def run_mnist_neural_network_app():
    # Thi·∫øt l·∫≠p MLflow
    mlflow_tracking_uri = "https://dagshub.com/huykibo/streamlit_mlflow.mlflow"
    try:
        os.environ["MLFLOW_TRACKING_USERNAME"] = st.secrets["mlflow"]["MLFLOW_TRACKING_USERNAME"]
        os.environ["MLFLOW_TRACKING_PASSWORD"] = st.secrets["mlflow"]["MLFLOW_TRACKING_PASSWORD"]
        mlflow.set_tracking_uri(mlflow_tracking_uri)
    except KeyError as e:
        st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y kh√≥a {e} trong st.secrets.")
        st.stop()

    EXPERIMENT_ID = "5"
    client = MlflowClient()

    # T·∫£i d·ªØ li·ªáu MNIST
    if 'full_data' not in st.session_state:
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
        X_full = np.concatenate([X_train, X_test], axis=0)
        y_full = np.concatenate([y_train, y_test], axis=0)
        X_full = X_full.reshape(-1, 784).astype(np.float32)
        y_full = y_full.astype(np.int32)
        st.session_state['full_data'] = (X_full, y_full)

    st.title("Ph√¢n lo·∫°i Ch·ªØ s·ªë MNIST v·ªõi Neural Network")

    # CSS t√πy ch·ªânh
    st.markdown("""
        <style>
            .section-title { font-size: 1.5em; font-weight: bold; color: #2c3e50; margin-bottom: 10px; }
            .prediction-box { margin-top: 10px; padding: 10px; border: 1px solid #ccc; border-radius: 5px; background-color: #f9f9f9; }
            .mode-title { font-size: 1.2em; font-weight: bold; color: #2c3e50; margin-bottom: 10px; }
        </style>
    """, unsafe_allow_html=True)

    tabs = st.tabs(["Th√¥ng tin", "Ch·ªçn d·ªØ li·ªáu", "X·ª≠ l√Ω d·ªØ li·ªáu", "Chia d·ªØ li·ªáu", "Hu·∫•n luy·ªán/ƒê√°nh gi√°", "Demo d·ª± ƒëo√°n", "Th√¥ng tin hu·∫•n luy·ªán"])

        # Tab 1: Th√¥ng tin
    with tab_info:
        st.header("Gi·ªõi thi·ªáu v·ªÅ ·ª®ng d·ª•ng v√† M·∫°ng Neural Network")
        st.markdown("""
        Ch√†o b·∫°n! ƒê√¢y l√† ·ª©ng d·ª•ng ph√¢n lo·∫°i ch·ªØ s·ªë vi·∫øt tay t·ª´ t·∫≠p d·ªØ li·ªáu **MNIST** b·∫±ng **M·∫°ng n∆°-ron nh√¢n t·∫°o (Neural Network)**. H√£y kh√°m ph√° c√°c t√≠nh nƒÉng v√† c√°ch ho·∫°t ƒë·ªông c·ªßa n√≥ nh√©!
        """, unsafe_allow_html=True)

        st.subheader("Ch·ªçn th√¥ng tin ƒë·ªÉ xem")
        info_option = st.selectbox(
            "",
            [
                "·ª®ng d·ª•ng n√†y l√† g√¨ v√† m·ª•c ti√™u c·ªßa n√≥?",
                "T·∫≠p d·ªØ li·ªáu MNIST: ƒê·∫∑c ƒëi·ªÉm v√† √Ω nghƒ©a",
                "Neural Network ‚Äì M·∫°ng n∆°-ron nh√¢n t·∫°o",
            ],
            label_visibility="collapsed",
            help="Ch·ªçn ƒë·ªÉ xem chi ti·∫øt v·ªÅ ·ª©ng d·ª•ng, d·ªØ li·ªáu, ho·∫∑c m√¥ h√¨nh."
        )

        if info_option == "·ª®ng d·ª•ng n√†y l√† g√¨ v√† m·ª•c ti√™u c·ªßa n√≥?":
            with st.spinner("ƒêang t·∫£i th√¥ng tin..."):
                progress_bar = st.progress(0)
                status_text = st.empty()
                for i in range(0, 101, 10):
                    progress_bar.progress(i)
                    status_text.text(f"ƒêang t·∫£i th√¥ng tin... {i}%")
                    time.sleep(0.05)
                st.subheader("üìò 1. ·ª®ng d·ª•ng n√†y l√† g√¨ v√† m·ª•c ti√™u c·ªßa n√≥?")
                st.markdown("""
                ƒê√¢y l√† m·ªôt ·ª©ng d·ª•ng ph√¢n lo·∫°i ch·ªØ s·ªë vi·∫øt tay d·ª±a tr√™n t·∫≠p d·ªØ li·ªáu **MNIST**, s·ª≠ d·ª•ng **M·∫°ng n∆°-ron nh√¢n t·∫°o (Neural Network)**.  
                - **MNIST**: T·∫≠p d·ªØ li·ªáu g·ªìm $70,000$ ·∫£nh ch·ªØ s·ªë t·ª´ $0$ ƒë·∫øn $9$, m·ªói ·∫£nh k√≠ch th∆∞·ªõc $28 \\times 28$ pixel (t·ªïng c·ªông $784$ ƒë·∫∑c tr∆∞ng).  
                - **M·ª•c ti√™u**:  
                  - X√¢y d·ª±ng v√† hu·∫•n luy·ªán m·ªôt m·∫°ng n∆°-ron ƒë·ªÉ nh·∫≠n di·ªán ch√≠nh x√°c c√°c ch·ªØ s·ªë.  
                  - Cung c·∫•p c√¥ng c·ª• tr·ª±c quan ƒë·ªÉ h·ªçc t·∫≠p v√† ƒë√°nh gi√° hi·ªáu qu·∫£ c·ªßa thu·∫≠t to√°n.  

                **Th√¥ng tin c∆° b·∫£n**:  
                - **$784$ ƒë·∫∑c tr∆∞ng**: M·ªói ·∫£nh ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng vector $784$ chi·ªÅu (gi√° tr·ªã pixel t·ª´ $0$ ƒë·∫øn $255$).  
                - **$70,000$ m·∫´u**: T·ªïng s·ªë ·∫£nh, ƒë∆∞·ª£c chia th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra.  
                - **Nhi·ªám v·ª•**: D·ª± ƒëo√°n nh√£n ($0$-$9$) d·ª±a tr√™n ƒë·∫∑c tr∆∞ng pixel.  
                """, unsafe_allow_html=True)
                status_text.text("ƒê√£ t·∫£i xong! 100%")
                time.sleep(0.5)
                status_text.empty()
                progress_bar.empty()

        elif info_option == "T·∫≠p d·ªØ li·ªáu MNIST: ƒê·∫∑c ƒëi·ªÉm v√† √Ω nghƒ©a":
            with st.spinner("ƒêang t·∫£i th√¥ng tin..."):
                progress_bar = st.progress(0)
                status_text = st.empty()
                for i in range(0, 101, 10):
                    progress_bar.progress(i)
                    status_text.text(f"ƒêang t·∫£i th√¥ng tin... {i}%")
                    time.sleep(0.05)
                st.subheader("üìò 2. T·∫≠p d·ªØ li·ªáu MNIST: ƒê·∫∑c ƒëi·ªÉm v√† √Ω nghƒ©a")
                st.markdown("""
                **MNIST** l√† t·∫≠p d·ªØ li·ªáu chu·∫©n trong h·ªçc m√°y, ƒë∆∞·ª£c t·∫°o b·ªüi Yann LeCun v√† c√°c c·ªông s·ª±.  
                - **ƒê·∫∑c ƒëi·ªÉm**:  
                  - G·ªìm c√°c ·∫£nh ch·ªØ s·ªë vi·∫øt tay t·ª´ h·ªçc sinh trung h·ªçc v√† nh√¢n vi√™n ƒëi·ªÅu tra d√¢n s·ªë M·ªπ.  
                  - Chu·∫©n h√≥a th√†nh k√≠ch th∆∞·ªõc $28 \\times 28$ pixel, thang ƒë·ªô x√°m (gi√° tr·ªã t·ª´ $0$ ƒë·∫øn $255$).  

                **√ù nghƒ©a**:  
                - L√† b√†i to√°n c∆° b·∫£n ƒë·ªÉ ki·ªÉm tra kh·∫£ nƒÉng ph√¢n lo·∫°i c·ªßa c√°c m√¥ h√¨nh h·ªçc m√°y.  
                - ƒê∆°n gi·∫£n nh∆∞ng ƒë·ªß ph·ª©c t·∫°p ƒë·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng ph√¢n bi·ªát c√°c l·ªõp t∆∞∆°ng t·ª± (v√≠ d·ª•: "$4$" v√† "$9$").  
                - Ph√π h·ª£p cho c·∫£ ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu v√† nghi√™n c·ª©u m√¥ h√¨nh ph·ª©c t·∫°p.  
                """, unsafe_allow_html=True)

                st.subheader("üì∑ Minh h·ªça d·ªØ li·ªáu MNIST")
                st.markdown("""
                D∆∞·ªõi ƒë√¢y l√† ·∫£nh minh h·ªça $10$ ch·ªØ s·ªë t·ª´ $0$ ƒë·∫øn $9$ t·ª´ t·∫≠p d·ªØ li·ªáu MNIST ƒë·ªÉ b·∫°n h√¨nh dung. M·ªói ch·ªØ s·ªë ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng ma tr·∫≠n $28 \\times 28$ pixel.
                """, unsafe_allow_html=True)
                try:
                    mnist_image = Image.open("mnist.png")
                    st.image(mnist_image, caption="·∫¢nh minh h·ªça $10$ ch·ªØ s·ªë t·ª´ $0$ ƒë·∫øn $9$ trong MNIST", width=800)
                except FileNotFoundError:
                    st.error("Kh√¥ng t√¨m th·∫•y file `mnist.png`. Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n.")
                except Exception as e:
                    st.error(f"L·ªói khi t·∫£i ·∫£nh: {e}")
                status_text.text("ƒê√£ t·∫£i xong! 100%")
                time.sleep(0.5)
                status_text.empty()
                progress_bar.empty()

        elif info_option == "Neural Network ‚Äì M·∫°ng n∆°-ron nh√¢n t·∫°o":
            with st.spinner("ƒêang t·∫£i th√¥ng tin..."):
                progress_bar = st.progress(0)
                status_text = st.empty()
                for i in range(0, 101, 10):
                    progress_bar.progress(i)
                    status_text.text(f"ƒêang t·∫£i th√¥ng tin... {i}%")
                    time.sleep(0.05)
                st.subheader("üìä 3. Neural Network ‚Äì M·∫°ng n∆°-ron nh√¢n t·∫°o")
                st.markdown("""
                **Neural Network (M·∫°ng n∆°-ron nh√¢n t·∫°o)** l√† m·ªôt m√¥ h√¨nh h·ªçc m√°y m√¥ ph·ªèng c√°ch ho·∫°t ƒë·ªông c·ªßa m·∫°ng n∆°-ron sinh h·ªçc trong n√£o ng∆∞·ªùi. N√≥ ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ h·ªçc c√°c ƒë·∫∑c tr∆∞ng ph·ª©c t·∫°p t·ª´ d·ªØ li·ªáu, ƒë·∫∑c bi·ªát hi·ªáu qu·∫£ v·ªõi b√†i to√°n nh·∫≠n di·ªán h√¨nh ·∫£nh nh∆∞ MNIST.
                """, unsafe_allow_html=True)

                st.subheader("üåê C·∫•u tr√∫c c∆° b·∫£n c·ªßa Neural Network")
                st.markdown("""
                - **L·ªõp ƒë·∫ßu v√†o (Input Layer)**: Nh·∫≠n d·ªØ li·ªáu th√¥ (v√≠ d·ª•: $784$ pixel t·ª´ ·∫£nh MNIST $28 \\times 28$).  
                - **L·ªõp ·∫©n (Hidden Layers)**: X·ª≠ l√Ω th√¥ng tin th√¥ng qua c√°c ph√©p t√≠nh tuy·∫øn t√≠nh v√† phi tuy·∫øn.  
                - **L·ªõp ƒë·∫ßu ra (Output Layer)**: ƒê∆∞a ra d·ª± ƒëo√°n (10 l·ªõp, t∆∞∆°ng ·ª©ng v·ªõi c√°c ch·ªØ s·ªë $0$-$9$).  
                """, unsafe_allow_html=True)

                st.subheader("üîß Quy tr√¨nh ho·∫°t ƒë·ªông")
                st.markdown("""
                Neural Network ho·∫°t ƒë·ªông qua c√°c b∆∞·ªõc sau, ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a d·ª±a tr√™n c√°c tham s·ªë b·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh trong tab **Hu·∫•n luy·ªán/ƒê√°nh gi√°**:
                """, unsafe_allow_html=True)

                st.subheader("1. Kh·ªüi t·∫°o m√¥ h√¨nh")
                st.markdown("""
                - X√°c ƒë·ªãnh c·∫•u tr√∫c m·∫°ng (s·ªë l·ªõp ·∫©n, s·ªë n∆°-ron m·ªói l·ªõp) v√† kh·ªüi t·∫°o **tr·ªçng s·ªë** ($W$) v√† **bias** ($b$) ng·∫´u nhi√™n (th∆∞·ªùng t·ª´ ph√¢n ph·ªëi Gaussian).  
                - **Tham s·ªë li√™n quan**: S·ªë l·ªõp ·∫©n, s·ªë n∆°-ron m·ªói l·ªõp.  
                - **Ch√∫ th√≠ch**:  
                  - $W$: Ma tr·∫≠n tr·ªçng s·ªë (weights) k·∫øt n·ªëi c√°c n∆°-ron gi·ªØa c√°c l·ªõp.  
                  - $b$: Vector bias (ƒë·ªô l·ªách) gi√∫p ƒëi·ªÅu ch·ªânh ƒë·∫ßu ra c·ªßa n∆°-ron.  
                - M·ª•c ƒë√≠ch: Thi·∫øt l·∫≠p c·∫•u tr√∫c ban ƒë·∫ßu ƒë·ªÉ b·∫Øt ƒë·∫ßu qu√° tr√¨nh h·ªçc.  
                """, unsafe_allow_html=True)
                try:
                    st.image(os.path.join("plnw", "step1_init.png"), caption="Minh h·ªça: Kh·ªüi t·∫°o m√¥ h√¨nh", width=600)
                except FileNotFoundError:
                    st.error("Kh√¥ng t√¨m th·∫•y ·∫£nh minh h·ªça cho B∆∞·ªõc 1.")
                except Exception as e:
                    st.error(f"L·ªói khi t·∫£i ·∫£nh: {e}")

                st.subheader("2. Lan truy·ªÅn thu·∫≠n (Feedforward)")
                st.markdown("""
                - T√≠nh to√°n ƒë·∫ßu ra d·ª± ƒëo√°n ($\\hat{Y}$) t·ª´ ƒë·∫ßu v√†o $X$ qua c√°c l·ªõp:  
                  $$ Z^{(l)} = A^{(l-1)} \\cdot W^{(l)} + b^{(l)} $$  
                  $$ A^{(l)} = \\text{h√†m k√≠ch ho·∫°t}(Z^{(l)}) $$  
                - **Ch√∫ th√≠ch**:  
                  - $Z^{(l)}$: T·ªïng tr·ªçng s·ªë ƒë·∫ßu v√†o t·∫°i l·ªõp $l$ (tr∆∞·ªõc khi √°p d·ª•ng h√†m k√≠ch ho·∫°t).  
                  - $A^{(l-1)}$: ƒê·∫ßu ra c·ªßa l·ªõp tr∆∞·ªõc ($l-1$), l√† ƒë·∫ßu v√†o c·ªßa l·ªõp $l$.  
                  - $W^{(l)}$: Ma tr·∫≠n tr·ªçng s·ªë c·ªßa l·ªõp $l$.  
                  - $b^{(l)}$: Vector bias c·ªßa l·ªõp $l$.  
                  - $A^{(l)}$: ƒê·∫ßu ra c·ªßa l·ªõp $l$ sau khi √°p d·ª•ng h√†m k√≠ch ho·∫°t.  
                - M·ª•c ƒë√≠ch: T·∫°o d·ª± ƒëo√°n ban ƒë·∫ßu t·ª´ d·ªØ li·ªáu ƒë·∫ßu v√†o qua c√°c l·ªõp n∆°-ron.  
                """, unsafe_allow_html=True)
                try:
                    st.image(os.path.join("plnw", "step2_feedforward.png"), caption="Minh h·ªça: Lan truy·ªÅn thu·∫≠n", width=600)
                except FileNotFoundError:
                    st.error("Kh√¥ng t√¨m th·∫•y ·∫£nh minh h·ªça cho B∆∞·ªõc 2.")
                except Exception as e:
                    st.error(f"L·ªói khi t·∫£i ·∫£nh: {e}")

                st.subheader("3. T√≠nh h√†m m·∫•t m√°t (Loss Function)")
                st.markdown("""
                - ƒêo ƒë·ªô sai l·ªách gi·ªØa d·ª± ƒëo√°n ($\\hat{Y}$) v√† nh√£n th·ª±c ($Y$) b·∫±ng **Cross-Entropy**:  
                  $$ L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=0}^{9} y_{ij} \\cdot \\log(\\hat{y}_{ij}) $$  
                - **Ch√∫ th√≠ch**:  
                  - $L$: Gi√° tr·ªã m·∫•t m√°t (loss) t·ªïng th·ªÉ c·ªßa m√¥ h√¨nh.  
                  - $N$: S·ªë l∆∞·ª£ng m·∫´u trong t·∫≠p d·ªØ li·ªáu.  
                  - $y_{ij}$: Gi√° tr·ªã th·ª±c t·∫ø (1 n·∫øu m·∫´u $i$ thu·ªôc l·ªõp $j$, 0 n·∫øu kh√¥ng).  
                  - $\\hat{y}_{ij}$: X√°c su·∫•t d·ª± ƒëo√°n b·ªüi m√¥ h√¨nh cho m·∫´u $i$ thu·ªôc l·ªõp $j$.  
                - M·ª•c ƒë√≠ch: ƒê·ªãnh l∆∞·ª£ng sai l·ªách ƒë·ªÉ ƒëi·ªÅu ch·ªânh m√¥ h√¨nh trong b∆∞·ªõc ti·∫øp theo.  
                """, unsafe_allow_html=True)
                try:
                    st.image(os.path.join("plnw", "step3_loss.png"), caption="Minh h·ªça: T√≠nh h√†m m·∫•t m√°t", width=600)
                except FileNotFoundError:
                    st.error("Kh√¥ng t√¨m th·∫•y ·∫£nh minh h·ªça cho B∆∞·ªõc 3.")
                except Exception as e:
                    st.error(f"L·ªói khi t·∫£i ·∫£nh: {e}")

                st.subheader("4. Lan truy·ªÅn ng∆∞·ª£c (Backpropagation)")
                st.markdown("""
                - T√≠nh ƒë·∫°o h√†m c·ªßa $L$ ƒë·ªÉ c·∫≠p nh·∫≠t $W^{(l)}$ v√† $b^{(l)}$ nh·∫±m gi·∫£m sai s·ªë d·ª± ƒëo√°n.  
                - **Ch√∫ th√≠ch**:  
                  - $\\frac{\\partial L}{\\partial W^{(l)}}$: ƒê·∫°o h√†m ri√™ng c·ªßa m·∫•t m√°t $L$ theo tr·ªçng s·ªë $W^{(l)}$.  
                  - $\\frac{\\partial L}{\\partial b^{(l)}}$: ƒê·∫°o h√†m ri√™ng c·ªßa m·∫•t m√°t $L$ theo bias $b^{(l)}$.  
                - M·ª•c ƒë√≠ch: X√°c ƒë·ªãnh h∆∞·ªõng ƒëi·ªÅu ch·ªânh tham s·ªë d·ª±a tr√™n sai s·ªë.  
                """, unsafe_allow_html=True)
                try:
                    st.image(os.path.join("plnw", "step4_backprop.png"), caption="Minh h·ªça: Lan truy·ªÅn ng∆∞·ª£c", width=600)
                except FileNotFoundError:
                    st.error("Kh√¥ng t√¨m th·∫•y ·∫£nh minh h·ªça cho B∆∞·ªõc 4.")
                except Exception as e:
                    st.error(f"L·ªói khi t·∫£i ·∫£nh: {e}")

                st.subheader("5. C·∫≠p nh·∫≠t tham s·ªë (Gradient Descent)")
                st.markdown("""
                - ƒêi·ªÅu ch·ªânh $W^{(l)}$ v√† $b^{(l)}$ ƒë·ªÉ gi·∫£m m·∫•t m√°t:  
                  $$ W^{(l)} = W^{(l)} - \\eta \\cdot \\frac{\\partial L}{\\partial W^{(l)}} $$  
                  $$ b^{(l)} = b^{(l)} - \\eta \\cdot \\frac{\\partial L}{\\partial b^{(l)}} $$  
                - **Ch√∫ th√≠ch**:  
                  - $\\eta$: T·ªëc ƒë·ªô h·ªçc (learning rate), ki·ªÉm so√°t m·ª©c ƒë·ªô thay ƒë·ªïi c·ªßa $W$ v√† $b$.  
                  - $\\frac{\\partial L}{\\partial W^{(l)}}$: Gradient c·ªßa $L$ theo $W^{(l)}$.  
                  - $\\frac{\\partial L}{\\partial b^{(l)}}$: Gradient c·ªßa $L$ theo $b^{(l)}$.  
                - M·ª•c ƒë√≠ch: T·ªëi ∆∞u h√≥a tham s·ªë ƒë·ªÉ gi·∫£m sai s·ªë d·ª± ƒëo√°n.  
                """, unsafe_allow_html=True)
                try:
                    st.image(os.path.join("plnw", "step5_gradient.png"), caption="Minh h·ªça: C·∫≠p nh·∫≠t tham s·ªë", width=600)
                except FileNotFoundError:
                    st.error("Kh√¥ng t√¨m th·∫•y ·∫£nh minh h·ªça cho B∆∞·ªõc 5.")
                except Exception as e:
                    st.error(f"L·ªói khi t·∫£i ·∫£nh: {e}")

                st.subheader("6. L·∫∑p l·∫°i")
                st.markdown("""
                - L·∫∑p l·∫°i t·ª´ b∆∞·ªõc 2 qua nhi·ªÅu **epoch** cho ƒë·∫øn khi m·∫•t m√°t $L$ h·ªôi t·ª•.  
                - **Ch√∫ th√≠ch**:  
                  - **Epoch**: M·ªôt l·∫ßn l·∫∑p qua to√†n b·ªô t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán.  
                - M·ª•c ƒë√≠ch: Tinh ch·ªânh m√¥ h√¨nh qua nhi·ªÅu v√≤ng l·∫∑p ƒë·ªÉ ƒë·∫°t hi·ªáu su·∫•t t·ªëi ∆∞u.  
                """, unsafe_allow_html=True)
                try:
                    st.image(os.path.join("plnw", "step6_repeat_improved.png"), caption="Minh h·ªça: L·∫∑p l·∫°i", width=600)
                except FileNotFoundError:
                    st.error("Kh√¥ng t√¨m th·∫•y ·∫£nh minh h·ªça cho B∆∞·ªõc 6.")
                except Exception as e:
                    st.error(f"L·ªói khi t·∫£i ·∫£nh: {e}")

                st.subheader("üîß C√°c tham s·ªë hu·∫•n luy·ªán: √ù nghƒ©a, ho·∫°t ƒë·ªông v√† c√¥ng th·ª©c")
                st.markdown("""
                D∆∞·ªõi ƒë√¢y l√† c√°c tham s·ªë ch√≠nh trong qu√° tr√¨nh hu·∫•n luy·ªán Neural Network, √Ω nghƒ©a c·ªßa ch√∫ng, c√°ch ho·∫°t ƒë·ªông v√† c√¥ng th·ª©c (n·∫øu c√≥):

                1. **S·ªë l·ªõp ·∫©n (Number of Hidden Layers):**  
                   - **√ù nghƒ©a**: Quy·∫øt ƒë·ªãnh ƒë·ªô s√¢u c·ªßa m·∫°ng, ·∫£nh h∆∞·ªüng ƒë·∫øn kh·∫£ nƒÉng h·ªçc c√°c ƒë·∫∑c tr∆∞ng ph·ª©c t·∫°p.  
                   - **Ho·∫°t ƒë·ªông**: TƒÉng s·ªë l·ªõp ·∫©n gi√∫p m·∫°ng h·ªçc ƒë∆∞·ª£c c√°c ƒë·∫∑c tr∆∞ng c·∫•p cao h∆°n, nh∆∞ng qu√° nhi·ªÅu l·ªõp c√≥ th·ªÉ g√¢y kh√≥ h·ªôi t·ª• ho·∫∑c overfitting.  
                   - **C√¥ng th·ª©c**: Kh√¥ng c√≥ c√¥ng th·ª©c c·ª• th·ªÉ, th∆∞·ªùng ƒë∆∞·ª£c ch·ªçn d·ª±a tr√™n kinh nghi·ªám ho·∫∑c th·ª≠ nghi·ªám (trong ·ª©ng d·ª•ng n√†y: t·ª´ 1 ƒë·∫øn 5).  

                2. **S·ªë n∆°-ron m·ªói l·ªõp ·∫©n (Number of Neurons per Layer):**  
                   - **√ù nghƒ©a**: Quy·∫øt ƒë·ªãnh ƒë·ªô r·ªông c·ªßa m·∫°ng, t·ª©c l√† kh·∫£ nƒÉng bi·ªÉu di·ªÖn th√¥ng tin trong m·ªói l·ªõp.  
                   - **Ho·∫°t ƒë·ªông**: Nhi·ªÅu n∆°-ron h∆°n gi√∫p m·∫°ng h·ªçc ƒë∆∞·ª£c nhi·ªÅu ƒë·∫∑c tr∆∞ng h∆°n, nh∆∞ng c≈©ng tƒÉng chi ph√≠ t√≠nh to√°n.  
                   - **C√¥ng th·ª©c**: Kh√¥ng c√≥, th∆∞·ªùng l√† l≈©y th·ª´a c·ªßa 2 (16, 32, 64, 128, v.v.) ƒë·ªÉ t·ªëi ∆∞u h√≥a ph·∫ßn c·ª©ng.  

                3. **T·ªëc ƒë·ªô h·ªçc (Learning Rate - Œ∑):**  
                   - **√ù nghƒ©a**: ƒêi·ªÅu ch·ªânh m·ª©c ƒë·ªô thay ƒë·ªïi c·ªßa tr·ªçng s·ªë trong m·ªói l·∫ßn c·∫≠p nh·∫≠t.  
                   - **Ho·∫°t ƒë·ªông**: Gi√° tr·ªã nh·ªè (v√≠ d·ª•: 0.0001) l√†m m√¥ h√¨nh h·ªçc ch·∫≠m nh∆∞ng ·ªïn ƒë·ªãnh; gi√° tr·ªã l·ªõn (v√≠ d·ª•: 0.01) h·ªçc nhanh h∆°n nh∆∞ng d·ªÖ v∆∞·ª£t qua ƒëi·ªÉm t·ªëi ∆∞u.  
                   - **C√¥ng th·ª©c**:  
                     $$ W_{t+1} = W_t - \\eta \\cdot \\frac{\\partial L}{\\partial W_t} $$  
                     - $W_{t+1}$: Tr·ªçng s·ªë sau khi c·∫≠p nh·∫≠t.  
                     - $W_t$: Tr·ªçng s·ªë t·∫°i b∆∞·ªõc hi·ªán t·∫°i.  
                     - $\\eta$: T·ªëc ƒë·ªô h·ªçc.  
                     - $\\frac{\\partial L}{\\partial W_t}$: Gradient c·ªßa m·∫•t m√°t theo tr·ªçng s·ªë.  

                4. **S·ªë l·∫ßn l·∫∑p (Epochs):**  
                   - **√ù nghƒ©a**: S·ªë l·∫ßn to√†n b·ªô d·ªØ li·ªáu hu·∫•n luy·ªán ƒë∆∞·ª£c ƒë∆∞a qua m·∫°ng.  
                   - **Ho·∫°t ƒë·ªông**: TƒÉng s·ªë l·∫ßn l·∫∑p gi√∫p m·∫°ng h·ªçc t·ªët h∆°n, nh∆∞ng qu√° nhi·ªÅu c√≥ th·ªÉ d·∫´n ƒë·∫øn overfitting.  
                   - **C√¥ng th·ª©c**: Kh√¥ng c√≥, l√† tham s·ªë ng∆∞·ªùi d√πng ch·ªçn (trong ·ª©ng d·ª•ng n√†y: 10-200).  

                5. **K√≠ch th∆∞·ªõc batch (Batch Size):**  
                   - **√ù nghƒ©a**: S·ªë m·∫´u ƒë∆∞·ª£c x·ª≠ l√Ω tr∆∞·ªõc khi c·∫≠p nh·∫≠t tr·ªçng s·ªë.  
                   - **Ho·∫°t ƒë·ªông**: Batch nh·ªè (v√≠ d·ª•: 16) gi√∫p c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n h∆°n nh∆∞ng ch·∫≠m; batch l·ªõn (v√≠ d·ª•: 512) nhanh h∆°n nh∆∞ng c·∫ßn nhi·ªÅu b·ªô nh·ªõ.  
                   - **C√¥ng th·ª©c**: Kh√¥ng c√≥, th∆∞·ªùng l√† l≈©y th·ª´a c·ªßa 2 ƒë·ªÉ t·ªëi ∆∞u h√≥a t√≠nh to√°n.  

                6. **H√†m k√≠ch ho·∫°t (Activation Function):**  
                   - **√ù nghƒ©a**: Quy·∫øt ƒë·ªãnh c√°ch n∆°-ron "k√≠ch ho·∫°t" ƒë·∫ßu ra d·ª±a tr√™n ƒë·∫ßu v√†o.  
                   - **Ho·∫°t ƒë·ªông**: Chuy·ªÉn ƒë·ªïi ƒë·∫ßu ra tuy·∫øn t√≠nh th√†nh phi tuy·∫øn ƒë·ªÉ m·∫°ng h·ªçc ƒë∆∞·ª£c c√°c ƒë·∫∑c tr∆∞ng ph·ª©c t·∫°p.  
                   - **Chi ti·∫øt c√°c h√†m k√≠ch ho·∫°t ph·ªï bi·∫øn:**  
                     - **ReLU (Rectified Linear Unit):**  
                       - **√ù nghƒ©a**: ƒê∆°n gi·∫£n, nhanh, tr√°nh v·∫•n ƒë·ªÅ bi·∫øn m·∫•t gradient.  
                       - **Ho·∫°t ƒë·ªông**: Ch·ªâ cho ph√©p c√°c gi√° tr·ªã d∆∞∆°ng ƒëi qua, ƒë·∫∑t gi√° tr·ªã √¢m v·ªÅ 0.  
                       - **C√¥ng th·ª©c**:  
                         $$ f(x) = \\max(0, x) $$  
                         - $x$: ƒê·∫ßu v√†o c·ªßa h√†m.  
                     - **Tanh (Hyperbolic Tangent):**  
                       - **√ù nghƒ©a**: Chu·∫©n h√≥a ƒë·∫ßu ra v·ªÅ kho·∫£ng [-1, 1], ph√π h·ª£p khi c·∫ßn c√¢n b·∫±ng gi√° tr·ªã √¢m/d∆∞∆°ng.  
                       - **Ho·∫°t ƒë·ªông**: T·∫°o ƒë·∫ßu ra phi tuy·∫øn, nh∆∞ng d·ªÖ g·∫∑p v·∫•n ƒë·ªÅ bi·∫øn m·∫•t gradient v·ªõi m·∫°ng s√¢u.  
                       - **C√¥ng th·ª©c**:  
                         $$ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$  
                         - $x$: ƒê·∫ßu v√†o c·ªßa h√†m.  
                     - **Softmax:**  
                       - **√ù nghƒ©a**: D√πng ·ªü l·ªõp ƒë·∫ßu ra ƒë·ªÉ chuy·ªÉn ƒë·ªïi th√†nh x√°c su·∫•t cho ph√¢n lo·∫°i ƒëa l·ªõp.  
                       - **Ho·∫°t ƒë·ªông**: Chu·∫©n h√≥a t·ªïng c√°c ƒë·∫ßu ra th√†nh 1, gi√∫p d·ª± ƒëo√°n l·ªõp c√≥ x√°c su·∫•t cao nh·∫•t.  
                       - **C√¥ng th·ª©c**:  
                         $$ f(x_i) = \\frac{e^{x_i}}{\\sum_{j=0}^{k} e^{x_j}} $$  
                         - $x_i$: ƒê·∫ßu v√†o c·ªßa n∆°-ron th·ª© $i$.  
                         - $k$: S·ªë l·ªõp (·ªü ƒë√¢y l√† 10).  

                7. **Tr√¨nh t·ªëi ∆∞u (Optimizer):**  
                   - **√ù nghƒ©a**: Thu·∫≠t to√°n ƒëi·ªÅu ch·ªânh tr·ªçng s·ªë ƒë·ªÉ gi·∫£m h√†m m·∫•t m√°t.  
                   - **Ho·∫°t ƒë·ªông**: Quy·∫øt ƒë·ªãnh c√°ch m·∫°ng h·ªôi t·ª• ƒë·∫øn ƒëi·ªÉm t·ªëi ∆∞u.  
                   - **V√≠ d·ª• ph·ªï bi·∫øn:**  
                     - **Adam**: K·∫øt h·ª£p ƒë·ªông l∆∞·ª£ng v√† RMSProp, th√≠ch nghi v·ªõi t·ªëc ƒë·ªô h·ªçc, nhanh v√† hi·ªáu qu·∫£.  
                     - **SGD (Stochastic Gradient Descent)**: C·∫≠p nh·∫≠t tr·ªçng s·ªë d·ª±a tr√™n gradient, ƒë∆°n gi·∫£n nh∆∞ng ch·∫≠m h∆°n Adam.  
                   - **C√¥ng th·ª©c (SGD)**:  
                     $$ W_{t+1} = W_t - \\eta \\cdot \\frac{\\partial L}{\\partial W_t} $$  
                     - $W_{t+1}$: Tr·ªçng s·ªë sau khi c·∫≠p nh·∫≠t.  
                     - $W_t$: Tr·ªçng s·ªë t·∫°i b∆∞·ªõc hi·ªán t·∫°i.  
                     - $\\eta$: T·ªëc ƒë·ªô h·ªçc.  
                     - $\\frac{\\partial L}{\\partial W_t}$: Gradient c·ªßa m·∫•t m√°t theo tr·ªçng s·ªë.  
                """, unsafe_allow_html=True)

                st.subheader("üåü ∆Øu ƒëi·ªÉm v√† nh∆∞·ª£c ƒëi·ªÉm c·ªßa Neural Network")
                st.markdown("""
                #### **∆Øu ƒëi·ªÉm:**  
                - **Kh·∫£ nƒÉng h·ªçc phi tuy·∫øn t√≠nh**: Neural Network c√≥ th·ªÉ h·ªçc c√°c m·ªëi quan h·ªá ph·ª©c t·∫°p, phi tuy·∫øn t√≠nh trong d·ªØ li·ªáu m√† c√°c m√¥ h√¨nh tuy·∫øn t√≠nh kh√¥ng l√†m ƒë∆∞·ª£c.  
                - **Kh·∫£ nƒÉng m·ªü r·ªông**: C√≥ th·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn v√† nhi·ªÅu chi·ªÅu (nh∆∞ ·∫£nh, √¢m thanh) khi ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë√∫ng c√°ch.  
                - **T√≠nh linh ho·∫°t**: C√≥ th·ªÉ √°p d·ª•ng cho nhi·ªÅu b√†i to√°n kh√°c nhau (ph√¢n lo·∫°i, h·ªìi quy, nh·∫≠n di·ªán h√¨nh ·∫£nh, v.v.).  
                - **T·ª± ƒë·ªông h·ªçc ƒë·∫∑c tr∆∞ng**: Kh√¥ng c·∫ßn tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng th·ªß c√¥ng, m·∫°ng t·ª± ƒë·ªông h·ªçc t·ª´ d·ªØ li·ªáu th√¥.  

                #### **Nh∆∞·ª£c ƒëi·ªÉm:**  
                - **ƒê√≤i h·ªèi t√†i nguy√™n l·ªõn**: C·∫ßn nhi·ªÅu d·ªØ li·ªáu v√† s·ª©c m·∫°nh t√≠nh to√°n (CPU/GPU) ƒë·ªÉ hu·∫•n luy·ªán hi·ªáu qu·∫£.  
                - **Kh√≥ gi·∫£i th√≠ch**: M·∫°ng ho·∫°t ƒë·ªông nh∆∞ "h·ªôp ƒëen", kh√≥ hi·ªÉu t·∫°i sao l·∫°i ƒë∆∞a ra d·ª± ƒëo√°n c·ª• th·ªÉ.  
                - **D·ªÖ b·ªã overfitting**: N·∫øu kh√¥ng ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh t·ªët (v√≠ d·ª•: thi·∫øu d·ªØ li·ªáu ho·∫∑c kh√¥ng d√πng regularization), m√¥ h√¨nh c√≥ th·ªÉ h·ªçc qu√° m·ª©c d·ªØ li·ªáu hu·∫•n luy·ªán.  
                - **Th·ªùi gian hu·∫•n luy·ªán l√¢u**: ƒê·∫∑c bi·ªát v·ªõi m·∫°ng s√¢u ho·∫∑c d·ªØ li·ªáu l·ªõn.  
                """, unsafe_allow_html=True)

                status_text.text("ƒê√£ t·∫£i xong! 100%")
                time.sleep(0.5)
                status_text.empty()
                progress_bar.empty()

    # Tab 2: Ch·ªçn d·ªØ li·ªáu
    with tabs[1]:
        st.markdown('<div class="section-title">Ch·ªçn S·ªë l∆∞·ª£ng D·ªØ li·ªáu</div>', unsafe_allow_html=True)
        X_full, y_full = st.session_state['full_data']
        num_samples = st.number_input("S·ªë l∆∞·ª£ng m·∫´u:", min_value=1, max_value=len(X_full), value=1000)
        if st.button("X√°c nh·∫≠n"):
            indices = np.random.choice(len(X_full), size=num_samples, replace=False)
            st.session_state['data'] = (X_full[indices].copy(), y_full[indices].copy())
            st.session_state['optimal_params'] = get_optimal_params(num_samples)
            st.success(f"ƒê√£ ch·ªçn {num_samples} m·∫´u!")
            gc.collect()

    # Tab 3: X·ª≠ l√Ω d·ªØ li·ªáu
    with tabs[2]:
        st.markdown('<div class="section-title">X·ª≠ l√Ω D·ªØ li·ªáu</div>', unsafe_allow_html=True)
        if 'data' not in st.session_state:
            st.info("Ch·ªçn d·ªØ li·ªáu tr∆∞·ªõc.")
        else:
            X, y = st.session_state['data']
            if st.button("Chu·∫©n h√≥a d·ªØ li·ªáu"):
                X_norm = X / 255.0
                st.session_state["data_processed"] = (X_norm.copy(), y.copy())
                st.success("ƒê√£ chu·∫©n h√≥a d·ªØ li·ªáu!")
                gc.collect()

    # Tab 4: Chia d·ªØ li·ªáu
    with tabs[3]:
        st.markdown('<div class="section-title">Chia T·∫≠p D·ªØ li·ªáu</div>', unsafe_allow_html=True)
        if 'data' not in st.session_state:
            st.info("Ch·ªçn v√† x·ª≠ l√Ω d·ªØ li·ªáu tr∆∞·ªõc.")
        else:
            data_source = st.session_state.get('data_processed', st.session_state['data'])
            X, y = data_source
            test_pct = st.slider("T·ª∑ l·ªá Test (%)", 0, 50, 20)
            valid_pct = st.slider("T·ª∑ l·ªá Validation (%)", 0, 50, 20)
            test_size = test_pct / 100
            X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
            valid_size = (valid_pct / 100) / (1 - test_size) if test_size < 1 else 0
            X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=valid_size, random_state=42)
            if st.button("X√°c nh·∫≠n ph√¢n chia"):
                st.session_state['split_data'] = {
                    "X_train": X_train.copy(), "y_train": y_train.copy(),
                    "X_valid": X_valid.copy(), "y_valid": y_valid.copy(),
                    "X_test": X_test.copy(), "y_test": y_test.copy()
                }
                st.success("ƒê√£ chia d·ªØ li·ªáu!")
                gc.collect()

    # Tab 5: Hu·∫•n luy·ªán/ƒê√°nh gi√°
    with tabs[4]:
        st.markdown('<div class="section-title">Hu·∫•n luy·ªán v√† ƒê√°nh gi√°</div>', unsafe_allow_html=True)
        if 'split_data' not in st.session_state:
            st.info("Chia d·ªØ li·ªáu tr∆∞·ªõc.")
        else:
            split_data = st.session_state['split_data']
            X_train, y_train = split_data["X_train"], split_data["y_train"]
            X_valid, y_valid = split_data["X_valid"], split_data["y_valid"]
            X_test, y_test = split_data["X_test"], split_data["y_test"]

            num_samples = len(X_train)
            params = st.session_state.get("training_params", get_optimal_params(num_samples))

            st.subheader("C·∫•u h√¨nh M√¥ h√¨nh")
            num_hidden_layers = st.number_input("S·ªë l·ªõp ·∫©n", min_value=1, value=len(params["hidden_layer_sizes"]))
            hidden_sizes = [st.number_input(f"S·ªë n∆°-ron l·ªõp ·∫©n {i+1}", min_value=1, value=params["hidden_layer_sizes"][i] if i < len(params["hidden_layer_sizes"]) else 32) for i in range(num_hidden_layers)]
            params["hidden_layer_sizes"] = tuple(hidden_sizes)
            params["learning_rate"] = st.number_input("T·ªëc ƒë·ªô h·ªçc", min_value=0.00001, max_value=1.0, value=float(params["learning_rate"]))
            params["epochs"] = st.number_input("S·ªë l·∫ßn l·∫∑p", min_value=1, value=params["epochs"])
            params["batch_size"] = st.number_input("K√≠ch th∆∞·ªõc batch", min_value=1, value=params["batch_size"])
            st.session_state["training_params"] = params

            if st.button("B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán"):
                st.write("### X√°c nh·∫≠n Tham s·ªë")
                st.write(f"- S·ªë l·ªõp ·∫©n: {len(params['hidden_layer_sizes'])}")
                st.write(f"- S·ªë n∆°-ron: {params['hidden_layer_sizes']}")
                st.write(f"- T·ªëc ƒë·ªô h·ªçc: {params['learning_rate']}")
                st.write(f"- S·ªë l·∫ßn l·∫∑p: {params['epochs']}")
                st.write(f"- K√≠ch th∆∞·ªõc batch: {params['batch_size']}")
                if st.button("X√°c nh·∫≠n v√† Hu·∫•n luy·ªán"):
                    with st.spinner("ƒêang hu·∫•n luy·ªán..."):
                        model = models.Sequential([
                            layers.Input(shape=(784,)),
                            *[layers.Dense(neurons, activation="relu") for neurons in params["hidden_layer_sizes"]],
                            layers.Dense(10, activation='softmax')
                        ])
                        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=params["learning_rate"]),
                                      loss='sparse_categorical_crossentropy', metrics=['accuracy'])
                        history = model.fit(X_train, y_train, epochs=params["epochs"], batch_size=params["batch_size"],
                                            validation_data=(X_valid, y_valid), verbose=0)
                        y_valid_pred = np.argmax(model.predict(X_valid, verbose=0), axis=1)
                        y_test_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)
                        acc_valid = accuracy_score(y_valid, y_valid_pred)
                        acc_test = accuracy_score(y_test, y_test_pred)

                        # Ki·ªÉm tra overfitting
                        train_acc = history.history['accuracy'][-1]
                        val_acc = history.history['val_accuracy'][-1]
                        if train_acc - val_acc > 0.1:
                            st.warning("C·∫£nh b√°o: M√¥ h√¨nh c√≥ d·∫•u hi·ªáu overfitting.")

                        st.session_state['model'] = model
                        st.session_state['training_results'] = {
                            'accuracy_val': acc_valid, 'accuracy_test': acc_test,
                            'loss_history': history.history['loss'],
                            'val_accuracy_history': history.history['val_accuracy']
                        }
                        st.success("Hu·∫•n luy·ªán xong!")
                        tf.keras.backend.clear_session()
                        del model, history
                        gc.collect()

            # Hu·∫•n luy·ªán AutoML
            if st.button("Hu·∫•n luy·ªán AutoML"):
                with st.spinner("ƒêang t√¨m tham s·ªë t·ªëi ∆∞u..."):
                    def build_model(hp):
                        model = models.Sequential([
                            layers.Input(shape=(784,)),
                            *[layers.Dense(hp.Int(f'units_{i}', 32, 128, step=32), activation='relu') for i in range(hp.Int('num_layers', 1, 3))],
                            layers.Dense(10, activation='softmax')
                        ])
                        model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                                      loss='sparse_categorical_crossentropy', metrics=['accuracy'])
                        return model
                    tuner = kt.Hyperband(build_model, objective='val_accuracy', max_epochs=10)
                    tuner.search(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))
                    best_model = tuner.get_best_models(num_models=1)[0]
                    st.session_state['model'] = best_model
                    st.success("Hu·∫•n luy·ªán AutoML xong!")

    # Tab 6: Demo d·ª± ƒëo√°n
    with tabs[5]:
        st.markdown('<div class="section-title">Demo D·ª± ƒëo√°n</div>', unsafe_allow_html=True)
        if 'model' not in st.session_state:
            st.info("Hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc.")
        else:
            model = st.session_state['model']
            input_method = st.selectbox("Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p li·ªáu", ["T·∫£i ·∫£nh l√™n", "V·∫Ω tr·ª±c ti·∫øp"])
            is_normalized = 'data_processed' in st.session_state

            def preprocess_input(data):
                return data / 255.0 if not is_normalized else data

            if input_method == "T·∫£i ·∫£nh l√™n":
                st.write("**H∆∞·ªõng d·∫´n**: T·∫£i l√™n ·∫£nh ch·ªØ s·ªë (28x28, n·ªÅn ƒëen, n√©t tr·∫Øng).")
                uploaded_file = st.file_uploader("T·∫£i ·∫£nh", type=["png", "jpg"])
                if uploaded_file:
                    image = Image.open(uploaded_file).convert('L').resize((28, 28))
                    st.image(image, caption="·∫¢nh sau resize (28x28)", width=100)
                    if st.button("D·ª± ƒëo√°n"):
                        image_array = np.array(image, dtype=np.float32).reshape(1, 784)
                        if np.sum(image_array > 50) < 50:
                            st.warning("·∫¢nh kh√¥ng ch·ª©a ch·ªØ s·ªë. Th·ª≠ l·∫°i!")
                        else:
                            prediction = model.predict(preprocess_input(image_array), verbose=0)[0]
                            predicted_class = np.argmax(prediction)
                            st.markdown(f'<div class="prediction-box">D·ª± ƒëo√°n: {predicted_class}</div>', unsafe_allow_html=True)
                            gc.collect()

            elif input_method == "V·∫Ω tr·ª±c ti·∫øp":
                st.write("**H∆∞·ªõng d·∫´n**: V·∫Ω ch·ªØ s·ªë (n√©t tr·∫Øng tr√™n n·ªÅn ƒëen).")
                canvas_result = st_canvas(
                    stroke_width=20,
                    stroke_color="#FFFFFF",
                    background_color="#000000",
                    height=280,
                    width=280,
                    drawing_mode="freedraw",
                    key="canvas"
                )
                if canvas_result.image_data is not None:
                    image = Image.fromarray(canvas_result.image_data.astype('uint8'), 'RGBA').convert('L').resize((28, 28))
                    st.image(image, caption="·∫¢nh sau resize (28x28)", width=100)
                    if st.button("D·ª± ƒëo√°n"):
                        image_array = np.array(image).reshape(1, 784)
                        if np.sum(image_array > 50) < 50:
                            st.warning("H√¨nh v·∫Ω kh√¥ng ch·ª©a ch·ªØ s·ªë. Th·ª≠ l·∫°i!")
                        else:
                            prediction = model.predict(preprocess_input(image_array), verbose=0)[0]
                            predicted_class = np.argmax(prediction)
                            st.markdown(f'<div class="prediction-box">D·ª± ƒëo√°n: {predicted_class}</div>', unsafe_allow_html=True)
                            gc.collect()

    # Tab 7: Th√¥ng tin hu·∫•n luy·ªán
    with tabs[6]:
        st.markdown('<div class="section-title">Th√¥ng tin Hu·∫•n luy·ªán</div>', unsafe_allow_html=True)
        runs = client.search_runs(experiment_ids=[EXPERIMENT_ID], order_by=["attributes.start_time DESC"])
        if runs and 'training_results' in st.session_state:
            run_options = {run.info.run_id: run.data.tags.get('mlflow.runName', run.info.run_id) for run in runs}
            selected_runs = st.multiselect("Ch·ªçn run ƒë·ªÉ so s√°nh", list(run_options.values()))
            if selected_runs:
                acc_tests = []
                run_names = []
                for run_id in [k for k, v in run_options.items() if v in selected_runs]:
                    run = client.get_run(run_id)
                    acc_test = run.data.metrics.get('accuracy_test', st.session_state['training_results']['accuracy_test'])
                    acc_tests.append(acc_test * 100)
                    run_names.append(run.data.tags.get('mlflow.runName', run_id))
                fig, ax = plt.subplots()
                ax.bar(run_names, acc_tests)
                ax.set_ylabel("ƒê·ªô ch√≠nh x√°c Test (%)")
                ax.set_title("So s√°nh c√°c m√¥ h√¨nh")
                st.pyplot(fig)
                plt.close(fig)

if __name__ == "__main__":
    run_mnist_neural_network_app()